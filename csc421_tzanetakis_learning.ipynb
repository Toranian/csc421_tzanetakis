{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 18 - Learning \n",
    "\n",
    "### George Tzanetakis, University of Victoria \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKPLAN \n",
    "\n",
    "The section number is based on the 3th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **18.1**, **18.2**, and **Summary**\n",
    "2. Expected: Same as Basic plus **18.3**, **18.6**, **18.7**\n",
    "3. Advanced: All the chapter including bibligraphical and historical notes \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forms of learning  \n",
    "\n",
    "An agent is **learning** if it improves its performance on future tasks after making observations about the world. \n",
    "In this chapter, we focus on a simple type of learning problem which is given a collection of input-output pairs, learn \n",
    "a function that predicts the output for new inputs. Even though at first glance this seems like a simple learning problem \n",
    "it has a large number of applications. \n",
    "\n",
    "## Feedback to learn from \n",
    "\n",
    "There are *types of feedback* that determine the three main types of learning: \n",
    "\n",
    "* **Unsupervised learning:** the agent learns patterns in the input without any explicit feedback. The most common example of unsuprvised learning is **clustering**\n",
    "* **Reinforcement learning:** the agent learns from a series of reinforcements (rewards and punishments).\n",
    "* **Supervised learning:** the agent observes some example input/output pairs and learns a function that maps the input to output. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.87730851 3.03342278 3.03367002 3.30130075 3.49475262 2.87601533\n",
      " 2.8124366  3.23201194 2.82079448 3.10311373]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mu = 3.0 \n",
    "sigma = 0.2 \n",
    "s = np.random.normal(mu, sigma, 10)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.normal(mu, sigma, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Simple Naive Bayes binary classification example\n",
    "\n",
    "In this notebook I will show a very simple example of this idea. Hopefully this will give you some general intuition about this approach. Then you can review the specific book examples that are more complicated (learning Gaussian mixtures, Bayesian networks with hidden variables, and learning hmm parameters). We will end by showing the general mathematical notation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple binary classification problem with one continous attribute. For example this could be classifying whether some one is a professional basketball player or not based on their height. We can generate some synthetic data for this problem by simply sampling two Gaussian distribution. Let's say that professional basketball players have an average height of 190cm and the average height of other people is 175cm. For simiplicity we will consider they both have a standard deviation of 10cm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[199.75968195 189.92646826 170.58953612 184.79353926 167.70288742\n",
      " 204.15053178 180.35451204 186.32696733 195.07197518 191.74084877\n",
      " 170.61860955 198.2611092  170.76314257 179.69196607 194.55363215\n",
      " 197.20220132 203.30462549 202.69946957 206.08307046 203.82237005]\n",
      "[159.60989321 172.5329254  180.2067365  178.14166257 172.52732566\n",
      " 163.04047212 159.05407217 179.20246609 152.56537927 189.45078765\n",
      " 179.84704358 173.01565692 169.30023166 167.24402415 182.37192164\n",
      " 162.59358627 169.70705759 175.48461213 168.42411359 191.00937401]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# generate twenty samples of each class \n",
    "bball_samples = np.random.normal(190, 10, 20)\n",
    "other_samples = np.random.normal(175, 10, 20)\n",
    "print(bball_samples)\n",
    "print(other_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 1000 samples of each class and plot histogram \n",
    "\n",
    "bball_mean_height = 190 \n",
    "other_mean_height = 175 \n",
    "bball_samples = np.random.normal(bball_mean_height, 10, 1000)\n",
    "other_samples = np.random.normal(other_mean_height, 10, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "bins = np.linspace(150, 220, 100)\n",
    "\n",
    "pyplot.hist(bball_samples, bins, alpha=0.5, label='x')\n",
    "pyplot.hist(other_samples, bins, alpha=0.5, label='y')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see in the histogram the height-distribution and overlap. You can also see that there is an equal number of instances for each class and that the standard deviation is the same.\n",
    "\n",
    "Now suppose that you are just given the nba_samples and other_samples and told that these are labeled samples for training a Naive Bayes classifier. You also know that they both have a standard deviation of 10cm so we will keep that. In this case the only parameter we are trying to estimate is the mean of each class. So 𝜃=(𝜇𝑛𝑏𝑎,𝜇𝑜𝑡ℎ𝑒𝑟).\n",
    "\n",
    "Given this data the maximum-likelihood estimate for the means is easily obtained by taking the statistical mean of the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.3937109842116 175.0417562092532\n"
     ]
    }
   ],
   "source": [
    "mu_bball = np.mean(bball_samples)\n",
    "mu_other = np.mean(other_samples)\n",
    "print(mu_bball, mu_other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have \"learned\" a model we can use it to predict. Suppose you are given a test height - lets say 183cm. You can calcuate the $P(183/nba)$ and $P(183/other)$ by using the corresponding probability density functions characterized by $\\mu_{nba}$ and $\\sigma = 10$ and $\\mu_{other}$ and $\\sigma = 10$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03251930626980593 0.029065835062969926\n",
      "183 is more likely a professional basketball player\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "test_height = 183\n",
    "\n",
    "p_bball = norm(mu_bball, 10).pdf(test_height)\n",
    "p_other = norm(mu_other, 10).pdf(test_height)\n",
    "print(p_bball, p_other)\n",
    "\n",
    "if (p_bball > p_other): \n",
    "    print(str(test_height) + \" is more likely a professional basketball player\")\n",
    "else: \n",
    "    print(str(test_height) + \" is more likely NOT a professional basketball player\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Decision Trees \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **decision tree** represents a function that takes as input a vector of attribute values and return a **decision** - a single output value. The input and output values can be discrete or continuous. To make things simple we will focus initially on discrete attributes and a single binary output (true or false)(positive or negative). \n",
    "\n",
    "A decision tree makes a decision by performing a sequence of test. Each node in the tree corresponds to a test of the value of one of the input attributes $A_i$ and the branches from the node are labeled with the possible values of the attribute. Each leaf node in the tree specifies a value to be return by the function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think of a decision tree is as through propositional logic. The goal attribute is true if and only if the input attributes satisfy one of the paths leading to a leaf with value *true*: \n",
    "\n",
    "$$\n",
    "Goal <=> (Path_{1} \\lor Path_{2} \\lor \\dots)\n",
    "$$\n",
    "\n",
    "Each path is a conjuction of attribute-value tests required to follow the path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inducing decision trees from examples\n",
    "\n",
    "\n",
    "An instance or example that can be used to \"train\" a decision tree consists of an $(\\bf{x},y)$ pair, where $x$ is a vector of values for the input attributes, and $y$ is a single output value. This is called the target (or goal) attribute and corresponds to whether $WillWait$ is $true$ or $false$. \n",
    "\n",
    "<img src=\"images/decision_tree_attributes.png\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "<img src=\"images/decision_tree.png\" width=\"100%\"/>\n",
    "\n",
    "\n",
    "Ideally we would want the smallest possible tree that is consistent with the examples. However, this is an intractable problem as there are $2^{2^{n}}$ trees to consider. We can do a decent job using a greedy divide and conquer strategy. \n",
    "\n",
    "1. Choose a \"good\" attribute to split\n",
    "2. If all examples after a split are positive (or negative) then return the corresponding classification \n",
    "3. Create two subproblems based on the split\n",
    "4. Recursively repeat the process until there are no attributes left to split \n",
    "5. When there are are both positive and negative examples use plurality to make decision\n",
    "\n",
    "\n",
    "<img src=\"images/decision_tree_attribute_splits.png\" width=\"100%\"/>\n",
    "\n",
    "<img src=\"images/decision_tree_learning_algorithm.png\" width=\"100%\"/>\n",
    "\n",
    "<img src=\"images/induced_decision_tree.png\" width=\"100%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing attribute tests\n",
    "\n",
    "We need a formal measure of \"fairly good\" and \"really useless\" to implement the $IMPORTANCE$ function. We will use the notation of information gain, which is defined in term of **entropy**, the fundamental quantity in information theory. \n",
    "\n",
    "**Entropy** of random variable: \n",
    "\n",
    "$$ \n",
    "H(V) = \\sum_{k} P(v_k) \\log_{2} \\frac{1}{P(v_k)} = - \\sum_{k} P(v_k) \\log_2P(v_k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the entropy of a fair coin is 1 bit: \n",
    "$$\n",
    "H(Fair) = -(0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1\n",
    "$$\n",
    "\n",
    "If *Loaded* is a random coin with $0.99$ probability of being Head and $0.01$ probability of being tail we get: \n",
    "\n",
    "$$\n",
    "H(Loaded) = -(0.99 \\log_2 0.98 + 0.01 \\log_2 0.01) \\approx 0.08 bits\n",
    "$$\n",
    "\n",
    "Let's definte $B(q)$ as the entropy of a Boolean random variable that is true with probability $q$: \n",
    "$$\n",
    "B(q) = -\\left (q \\log_2 q + (1-q) \\log_2 (1 - q)\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training set contains $p$ positive examples and $n$ negative examples then the entropy of the target attribute on the whole set is: \n",
    "$$ \n",
    "H(Goal) = B\\left(\\frac{p}{p+n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example the restaurant training set has $p=n=6$ so the corresponding entropy is $B(0.5) = 1$. A test on a single attribute(feature) might give us only part of this 1 bit. We can measure exactly how much by looking at the entropy remaining after the attribute test. \n",
    "Suppose that an attribute A with $d$ distinct values divides the training set $E$ into subset $E_1, \\dots E_d$. Each subset $E_k$ has $p_k$ positive examples and $n_k$ negative examples so if we go along that branch we need an additional $B(p_k/(p_k +n_k)$ bits of information to answer the question. A randomly chosen example from the training set has the kth value for the attribute with probability $(p_k + n_k)/(p + n)$, so the expected entropy remaining after testing attribute $A$ is \n",
    "$$ \n",
    "Remainder(A) = \\sum_{k=1}^{d} \\frac{(p_k + n_k)}{p+n} B\\left(\\frac{p_k}{p_k+n_k}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **information gain** from the attribute test on $A$ is expected reduction in entropy: \n",
    "$$ \n",
    "Gain(A) = B\\left(\\frac{p}{p+n}\\right) - Remainder(A) \n",
    "$$\n",
    "\n",
    "For example consider the attributes from our example we have: \n",
    "\n",
    "$$ \n",
    "Gain(Patrons) = 1 - \\left[\\frac{2}{12} B\\left(\\frac{0}{2}\\right) + \\frac{4}{12}B\\left(\\frac{4}{4}\\right)+ \\frac{6}{12}B\\left(\\frac{2}{6}\\right)\\right] \\approx 0.541 bits\n",
    "$$\n",
    "$$ \n",
    "Gain(Type) = 1 - \\left[\\frac{2}{12} B\\left(\\frac{1}{2}\\right) + \\frac{2}{12}B\\left(\\frac{1}{2}\\right)+ \\frac{4}{12}B\\left(\\frac{2}{4}\\right) + \\frac{4}{12}B\\left(\\frac{2}{4}\\right)\\right] = 0  bits\n",
    "$$\n",
    "\n",
    "\n",
    "### Additional topics \n",
    "\n",
    "* Overfitting\n",
    "* Decision tree pruning\n",
    "* Signficance test\n",
    "* Early stopping\n",
    "* Missing data\n",
    "* Multi-valued attributes\n",
    "* Continuous and integer-valued input attributes\n",
    "* Continuous-valued output attributes\n",
    "\n",
    "One **important property** of decision trees is that it is possible for a human to understand the reason for the output of the learning algorithm. This can be a **legal requirement** for financial decisions and is not the case in other learned representations such as neural networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
