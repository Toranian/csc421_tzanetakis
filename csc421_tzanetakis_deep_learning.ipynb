{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 21 - Deep Learning \n",
    "### George Tzanetakis, University of Victoria \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **21.1**, **21.2**, , and **Summary**\n",
    "2. Expected: Same as Basic plus **21.3**\n",
    "3. Advanced: All the chapter including bibligraphical and historical notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "* All models are wrong. Some are useful - George Box\n",
    "* All AI systems are not intelligent. Some are useful - George Tzanetakis \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "There were many attempts to train and use ANNs with more\n",
    "than one hidden layer but due to various practical problems\n",
    "(including long training times and lack of large amounts of\n",
    "data) they did not become succsessful until around 2009-2010.\n",
    "Advances in GPUs enabled faster training and availability of\n",
    "larger amounts of training data resulted in a resurgence of\n",
    "interest in Neural Network architectures and they were shown\n",
    "to provide superior performance than existing state-of-the-art\n",
    "algorithms for a variety of tasks such as image classification in\n",
    "computer vision and automatic speech recognition. The last few years \n",
    "there has been  enormous interest by companies (Google\n",
    "Brain, Facebook AI) with a lot of excellent software being\n",
    "developed.\n",
    "\n",
    "\n",
    "<img src=\"images/hinton.png\" width=\"70%\"/>\n",
    "\n",
    "## 2018 Turing Award Winners\n",
    "\n",
    "<img src=\"images/turing-2018-bengio-hinton-lecun.png\" width=\"70%\"/>\n",
    "\n",
    "## Geoffrey Hinton\n",
    "\n",
    "* Backpropagation (1986)\n",
    "\n",
    "* Boltzmann Machines (1983) \n",
    "\n",
    "* Improvements to convolutional neural networks (2012) - deep learning (rectified linear neurons and dropout regularization), AlexNet and ImageNet\n",
    "\n",
    "## Yoshua Bengio\n",
    "\n",
    "* Probabilistic models of sequences\n",
    "\n",
    "* High-dimensional word embeddings and attention (2000)\n",
    "\n",
    "*  Generative adversarial networks (2010) with Ian Goodfellow\n",
    "\n",
    "## Yann LeCun\n",
    "\n",
    "* Convolutional neural networks (1980s)  \n",
    "\n",
    "* Broadening the vision of neural networks - learning systems can be built as complex networks of modules where backpropagation is performed through automatic differentiation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet \n",
    "\n",
    "\n",
    "AI researcher Fei-Fei Li began working on the idea for ImageNet in 2006. At a time when most AI research focused on models and algorithms, Li wanted to expand and improve the data available to train AI algorithms. In 2007, Li met with Princeton professor Christiane Fellbaum, one of the creators of WordNet, to discuss the project. As a result of this meeting, Li went on to build ImageNet starting from the word database of WordNet and using many of its features. \n",
    "\n",
    "ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500–1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total.\n",
    "\r\n",
    "As an assistant professor at Princeton, Li assembled a team of researchers to work on the ImageNet project. They used Amazon Mechanical Turk to help with the classification  images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "They presented their database for the first time as a poster at the 2009 Conference on Computer Vision and Pattern Recognition (CVPR) in Flora.\n",
    "\n",
    "\n",
    "Fei Fei Li \n",
    "\n",
    "[The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI](https://www.amazon.ca/Worlds-See-Curiosity-Exploration-Discovery-ebook/dp/B0BPQSLVL6) \n",
    "\n",
    "<img src=\"images/imagenet.png\" width=\"100%\"/>\n",
    "\n",
    "<img src=\"images/feifeili.png\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Understanding deep learing starts from the simple feed-forward netowrk with input layer, hidden layers, and output layer. Each node/unit in a layer is collected with weights to every node in the next layer. The resulting weighted sum is then processed by a nonlinear activition function to produce output. So one can think of the entire network as a sequence of applying vector-matrix multiplication followed by non-linar activations. \n",
    "We can add +1 to the input so that total weighted sum can be non-zero even when all the output of the previous layer/inputs of the current layer are all zero. \n",
    "\n",
    "\\begin{equation}\n",
    "a_j = g_j(\\mathbf{w}^T \\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf(w)$ is the vector of weights leading into unit $j$, and $\\mathbf x$ is the vector of inputs to unit $j$. \n",
    "\n",
    "Activation is non-linear otherwise any composition of units would still represent a linear function. The nonlinearity is what allows sufficiently large networks of units to represent arbitrary functions. \n",
    "\n",
    "\n",
    "There are different activation functions: \n",
    "\n",
    "1. Logistic or sigmoid \n",
    "\\begin{equation} \n",
    "\\sigma(x) = 1 / (1 + e^{-x})\n",
    "\\end{equation} \n",
    "2. Rectified linear unit \n",
    "\\begin{equation} \n",
    "ReLU(x) = max(0,x)\n",
    "\\end{equation} \n",
    "3. Softplus \n",
    "\\begin{equation} \n",
    "softplus(x) = log(1 + e^{x})\n",
    "\\end{equation} \n",
    "4. tanh \n",
    "\n",
    "\n",
    "Vector form of network: \n",
    "\\begin{equation} \n",
    "h_w(\\mathbf x) = g^{2}(\\mathbf{W}^{2}g^{1}(\\mathbf W^{(1)}\\mathbf x))\n",
    "\\end{equation} \n",
    "\n",
    "For supervised learning training we can use **gradient descent** i.e calculate of the gradient of the loss function with respect to the weights, and then adjust the weights along the gradient direction to reduce the loss. \n",
    "\n",
    "Example loss function where $y$ is the ground truth value and $\\hat y$ is the output prediction of the network. \n",
    "\\begin{equation} \n",
    "Loss(h_{w}) = (y-\\hat y)^2\n",
    "\\end{equation} \n",
    "\n",
    "The idea is that for each training sample (or mini-batch of samples) we can calcuate the gradient with respect to the loss function and propagate the error backwards from the output layer through the hidden layers, and eventually to the input layer. This algorithm is called **back propagtion**. \n",
    "\n",
    "\n",
    "## Input encoding \n",
    "\n",
    "Boolean attributes are typically encoded as $0$ for False and $1$ for True. Numeric attributes whether integer \n",
    "or real-valued are typically used as is or sometimes mapped onto a log-scale. Networks used with images have array-like internal structures that aim to reflect the semantics of adjacency of pixels. Categorial values are usually encoded with **one-hot-encoding** to avoid numerical adjacency issues. \n",
    "\n",
    "\n",
    "\n",
    "## Output layers and loss functions \n",
    "\n",
    "In most deep learning applications, it is more common to interpret the output value $\\mathbf{\\hat y}$ as probabilities and to use the **negative log likelihood** as the loss function. \n",
    "\n",
    "\\begin{equation} \n",
    "\\mathbf{w^*} = \\underset{w}{\\operatorname{argmin}} - \\sum_{j=1}^{N} logP_w(\\mathbf{y_j}|\\mathbf{x_j})\n",
    "\\end{equation} \n",
    "\n",
    "Without going into details this is called the cross-entropy loss in Deep Learning literature. \n",
    "For binary classification problems a **sigmoid output** layer will do what we want and output \n",
    "probabilities. For multiclass problems we can use a **softmax** layer which outputs a vector of non-negative \n",
    "numbers that sum up to 1. For regression problems we can use a linear output layer without any activation function. This corresponds to doing a classical linear regression at the output layer after the multiple non-linear transforms. \n",
    "\n",
    "## Hidden Layers \n",
    "\n",
    "From 1985-2010 internal nodes typically used sigmoid and tanh activation functions almost exclusively. From around 20210 onwards the ReLU and softplus become more popular, partly because they are believed to avoid the problem of vanishing gradients. Experiments suggest that deep and narrow networks learn better than shallow and wide given a fixed total number of weights. \n",
    "\n",
    "There is little understanding at the moment as to why some structures seem to work better than others for some particular problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "# CIFAR Canadian Institute for Advanced Research\n",
    "# 2009 Tiny images object classificatino - AlexNet \n",
    "# Reported training times from 2014 - 20 minutes \n",
    "\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('Training set')\n",
    "print(f'Samples: {trainset.data.shape}')\n",
    "print(f'Labels: {len(trainset.targets)}')\n",
    "\n",
    "print('\\nTest set')\n",
    "print(f'Samples: {testset.data.shape}')\n",
    "print(f'Labels: {len(testset.targets)}')\n",
    "\n",
    "print('\\nClasses\\n')\n",
    "print(tabulate(\n",
    "    list(trainset.class_to_idx.items()), headers=['Name', 'Index'], \n",
    "    tablefmt='orgtbl'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    " dev = \"cuda:0\" \n",
    "else: \n",
    " dev = \"cpu\" \n",
    "\n",
    "#dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "dev = \"cuda\" \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3,32,5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n",
    "print(net)\n",
    "\n",
    "summary(net, (3,32,32), batch_size=32, device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "for epoch in range(20):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device,non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 400 == 399:    # print every 400 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))\n",
    "\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "print(device)\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "net.to(device)\n",
    "images = images.to(device)\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# print(outputs)\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_accuracy(net, testloader, device):\n",
    "    correct = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images + 0.2 * torch.randn(images.shape).to(device)\n",
    "            \n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / len(testloader.dataset)\n",
    "    \n",
    "    \n",
    "def test_accuracy_per_class(net, testloader, device):\n",
    "    correct_pred = {classname: 0 for classname in trainset.classes}\n",
    "    total_pred = {classname: 0 for classname in trainset.classes}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(images)\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "\n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predicted):\n",
    "                if label == prediction:\n",
    "                    correct_pred[trainset.classes[label]] += 1\n",
    "                total_pred[trainset.classes[label]] += 1\n",
    "    \n",
    "    accuracy_per_class = {classname: 0 for classname in trainset.classes}\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = (100 * float(correct_count)) / total_pred[classname]\n",
    "        accuracy_per_class[classname] = accuracy\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "\n",
    "test_acc = test_accuracy(net, testloader, 'cuda')\n",
    "print(f'Best trial test set accuracy: {test_acc}')\n",
    "\n",
    "overall_accuracy = test_accuracy(net, testloader, dev)\n",
    "\n",
    "print(\n",
    "    'Overall accuracy of the network  '\n",
    "    f'{(overall_accuracy * 100):.2f} %\\n'\n",
    "    'on the 10000 test images'\n",
    ")\n",
    "\n",
    "accuracy_per_class = test_accuracy_per_class(net, testloader, dev)\n",
    "\n",
    "print('Accuracy per class\\n')\n",
    "for classname, accuracy in accuracy_per_class.items():\n",
    "    print(f'{classname:12s} {accuracy:.2f} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, metrics, svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Training set')\n",
    "print(f'Samples: {trainset.data.shape}')\n",
    "print(f'Labels: {len(trainset.targets)}')\n",
    "print(type(trainset.data))\n",
    "print(type(trainset.targets))\n",
    "print('\\nTest set')\n",
    "print(f'Samples: {testset.data.shape}')\n",
    "print(f'Labels: {len(testset.targets)}')\n",
    "\n",
    "train_n_samples = len(trainset.data)\n",
    "print(train_n_samples)\n",
    "test_n_samples = len(testset.data)\n",
    "print(test_n_samples)\n",
    "\n",
    "Xtrain = trainset.data\n",
    "Xtest  = testset.data\n",
    "\n",
    "\n",
    "\n",
    "from skimage.feature import hog\n",
    "Xtrain_hog = []\n",
    "for i in range(len(Xtrain)):\n",
    "    fd  = hog(Xtrain[i] , orientations=9 , pixels_per_cell = (8,8),\n",
    "                     cells_per_block = (2,2) , visualize = False, channel_axis=-1)\n",
    "    Xtrain_hog.append(fd)\n",
    "    if ((i % 10000) == 0): \n",
    "        print(i)\n",
    "\n",
    "Xtrain_hog = np.array(Xtrain_hog)\n",
    "print('Done calculating HOGs for training')\n",
    "print(Xtrain_hog.shape)\n",
    "\n",
    "Xtest_hog = []\n",
    "for i in range(len(Xtest)):\n",
    "    fd = hog(Xtest[i] , orientations=9 , pixels_per_cell = (8,8),\n",
    "                     cells_per_block = (2,2) , visualize = False, channel_axis=-1)\n",
    "    Xtest_hog.append(fd)\n",
    "    if ((i % 1000) == 0): \n",
    "        print(i)\n",
    "        \n",
    "Xtest_hog = np.array(Xtest_hog)\n",
    "print('Done calculating HOGs for testing')\n",
    "\n",
    "ytrain = trainset.targets\n",
    "ytest  = testset.targets\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(0.8)\n",
    "Xtrain_pca = pca.fit_transform(Xtrain_hog)\n",
    "Xtest_pca  = pca.transform(Xtest_hog)\n",
    "print(Xtrain_pca.shape)\n",
    "print(Xtest_pca.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(C=10, cache_size=10000)\n",
    "clf.fit(Xtrain_pca, ytrain)\n",
    "\n",
    "ytest_predict  = clf.predict(Xtest_pca)\n",
    "print(classification_report(ytest, ytest_predict))\n",
    "\n",
    "color = 'white'\n",
    "cm = confusion_matrix(ytest, ytest_predict)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time \n",
    "Xtrain_pca = Xtrain_pca[0:5000]\n",
    "ytrain = ytrain[0:5000] \n",
    "Xtest_pca = Xtest_pca[0:5000]\n",
    "ytest = ytest[0:5000]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Create a classifier: a support vector classifier\n",
    "clf = svm.SVC(C=10, cache_size=10000)\n",
    "clf.fit(Xtrain_pca, ytrain)\n",
    "\n",
    "ytest_predict  = clf.predict(Xtest_pca)\n",
    "print(classification_report(ytest, ytest_predict))\n",
    "\n",
    "color = 'white'\n",
    "cm = confusion_matrix(ytest, ytest_predict)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "Xtrain_pca = Xtrain_pca[0:5000]\n",
    "ytrain = ytrain[0:5000] \n",
    "Xtest_pca = Xtest_pca[0:5000]\n",
    "ytest = ytest[0:5000]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Create a classifier: a support vector classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain_pca, ytrain)\n",
    "\n",
    "ytest_predict  = gnb.predict(Xtest_pca)\n",
    "print(classification_report(ytest, ytest_predict))\n",
    "\n",
    "color = 'white'\n",
    "cm = confusion_matrix(ytest, ytest_predict)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "Xtrain_pca = Xtrain_hog[0:5000]\n",
    "ytrain = ytrain[0:5000] \n",
    "Xtest_pca = Xtest_hog[0:5000]\n",
    "ytest = ytest[0:5000]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Create a classifier: a support vector classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain_pca, ytrain)\n",
    "\n",
    "ytest_predict  = gnb.predict(Xtest_pca)\n",
    "print(classification_report(ytest, ytest_predict))\n",
    "\n",
    "color = 'white'\n",
    "cm = confusion_matrix(ytest, ytest_predict)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "\n",
    "Xtrain = Xtrain.reshape(-1, 32 * 32 * 3)\n",
    "Xtest = Xtest.reshape(-1, 32 * 32 * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "Xtrain_pca = Xtrain[0:5000]\n",
    "ytrain = ytrain[0:5000] \n",
    "Xtest_pca = Xtest[0:5000]\n",
    "ytest = ytest[0:5000]\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Create a classifier: a support vector classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(Xtrain_pca, ytrain)\n",
    "\n",
    "ytest_predict  = gnb.predict(Xtest_pca)\n",
    "print(classification_report(ytest, ytest_predict))\n",
    "\n",
    "color = 'white'\n",
    "cm = confusion_matrix(ytest, ytest_predict)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=['airplanes', 'cars', 'birds', 'cats', 'deer', 'dogs', 'frogs', 'horses', 'ships', 'trucks'])\n",
    "disp.plot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation using computers \n",
    "\n",
    "\n",
    "1. Symbolic differentiation: Automatic manipulation of mathematical expressions to get derivatives\n",
    "    * Input and output are mathematical expressions\n",
    "    * Used in Mathematica, Maple, Sympy, etc.\n",
    "    \n",
    "    \n",
    "    \n",
    "2.  Numeric differentiation: Approximating derivatives by finite differences: \n",
    "\n",
    "\\begin{equation} \n",
    "\\frac{\\partial f(x_1, \\dots, x_N)}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\dots, x_i+h, \\dots, x_N) - f(x_1, \\dots, x_i-h, \\dots, x_N)}{2h}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "3. Automatic differentiation (AD): A method to get exact derivatives efficiently, by storing information as you go forward that you can reuse as you go backwards\n",
    " \n",
    "    * Takes code that computes a function and returns code that computes the derivative of that function.\n",
    "    * “The goal isn’t to obtain closed-form solutions, but to be able to write a program that efficiently computes the derivatives.”\n",
    "     * Autograd, Torch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "\n",
    "Q = 3*a**3 - b**2\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "print(9*a**2)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import elementwise_grad as egrad\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-31.4,31.4, 256)\n",
    "sinc = lambda x: np.sin(x) / x\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title(\"sinc function and derivatives\", fontsize=24)\n",
    "my_fn = sinc\n",
    "\n",
    "for ii in range(5):\n",
    "    plt.plot(x, my_fn(x), lw=3, label=\"d{} sinc(x)/dx{}\".format(ii,ii))\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.axis([-32, 32, -0.50, 1.2])\n",
    "    my_fn = egrad(my_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable programming \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of computing using building blocks that can be differentiated orinated in Deep Learning but has found applications in other areas. For example it is possible to model chains of digital signal processing effects \n",
    "using differentiable DSP. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural networks \n",
    "\n",
    "In order to preserve local adjacency information, the first hidden layer should receive input from only a small, local region of the image. In addition, we would like approximate **spatial invariance** at small to moderate scales - i.e an edge or an eye should look the same if it appears in a different, small region of the image. \n",
    "\n",
    "Constraining the $l$ weights connecting a local region to a unit in the hidden layer to be the same for each hidden unit (i.e $w_{1,i}, \\dots w_{l,i}$ should be the same as $w_{1,j}, \\dots, w_{l, j}$ for hidden unit $i$ and $j$. A pattern of weights that is replicated across multiple local regions is called a **kernel** and the process of applying the **kernel** to the pixels of an image is called **convolution**. Note that in DSP this is called cross-correlation and convolution means something else. \n",
    "\n",
    "Let's illustrate **convolution** with an example in 1D. Consider the sequence $5,6,6,2,5,6,5$ and the **kernel** \n",
    "$+1,-1,+1$. This kernel will detect (produce higher values) when there is a \"dark\" pixel. The result of applying \n",
    "the kernel at the first position of the sequence will be: $5-6+6 = 5$. The kernel is then moved by **stride** pixels. For example if the **stride** is two the next output value will be $6-2+5=9$. If the stride is one the next output value will be $6-6+2=2$. \n",
    "\n",
    "Convolution is a linear operation and therefore we can propagate gradients through it, just like we did with fully connected networks. \n",
    "\n",
    "\n",
    "CNNs were inspired originally by models of the visual cortex proposed in neuroscience. In those models, the **receptive field** of a neuron is the portion of the sensory input that can affect that neuron's activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of 1D convolution with kernel size 3 and stride 2 \n",
    "\n",
    "a = np.array([[+1,-1,+1,0,0,0,0],[0,0,+1,-1,+1,0,0], [0,0,0,0,+1,-1,+1]])\n",
    "\n",
    "b = np.array([[5,6,6,2,5,6,5]])\n",
    "c = np.matmul(a,b.T)\n",
    "print(a)\n",
    "print(b.T)\n",
    "print('=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layer \n",
    "\n",
    "A **pooling** layer in a neural network summarizes a set of adjacent units from the preceding layer with a single value. **Average-pooling** computes the average value of its $l$ inputs. Average pooling facilitates multiscale recognition. It also reduces the number of weights required in subsequent layers, leading to lower computational cost and prossibly faster learning. **Max pooling** computes the maximum value of its $l$ inputs. Max-pooling acts as a kind of logical disjunction, saying that a feature exists somewhere in the unit's receptive field. \n",
    "\n",
    "In a image classification network the final layer of the network will be a softmax with $c$ output units. The early layers of the CNN are image-sized, so somewhere in between there must be significant reductions in layer size. Convolutional layers and pooling layers with stride larger than 1 all serve to reduce a layer size. \n",
    "\n",
    "\n",
    "### Tensors \n",
    "\n",
    "Multi-dimensional arrays of any dimension - they keep track of the \"shape\" of the data as it progress through layers of the network. Describing the CNN in terms of tensors and tensor operators, a deep learning package can generate compiled code that is highly optimized for the underlying computational substrate. \n",
    "\n",
    "\n",
    "Support we are traying on $256 \\times 256$ RGB images with a minibatch size of $64$. The input will be a 4-dimensional tensor of size $256 \\times 256 \\times 3 \\times 64$. The we applyu 96 kernel of size $5 \\times 5 \\times 3$ with a stride of $2$ in both $x$ and $y$ dimensions. This gives as an output tensor of size \n",
    "$128 \\times 128 \\times 96 \\times 64$. Such a tensor is called a **feature map** - note no dedicated color \n",
    "channels but color information has been incorporated if the learning algorithm finds it useful for the final \n",
    "predictions of the network. \n",
    "\n",
    "Graphical Processing units (GPUs) are specialized hardware for graphics operations that can be used \n",
    "to perform tensor operations. Tensor processing units (TPUs) are specialized hardware for computing tensor \n",
    "operations that optimize for speed and throughput rather than high numerical precision. \n",
    "\n",
    "### Residual networks \n",
    "\n",
    "In 2012 it was shown for the first time that a DNN (AlexNet) with 8 neural network layers (5 convolutional and 3 full-connected) was more successful than traditional, hand-crafted feature learning on ImageNet. The success of DNNs is found in these additional layers. It is thought that these layers progressively learn more complex features (for example the first layer might learn edges, the second layer might learn shapes, the third layer might learn objects, etc). \n",
    "\n",
    "Networks with many layers suffer from the vanishing/exploding gradient problem. Some of that can be alleviated by batch normalization. Another approach is to user residual blocks that contain skip connections. Skip connection append the output of a layer (with a potential dimension adjustment) by concatenating it with output of subsequent layers. \n",
    "\n",
    "The authors of the ResNet architecture test their network with 100 and 1,000 layers on the CIFAR-10 dataset. They tested on the ImageNet dataset with 152 layers, another very popular Deep CNN architecture.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithms \n",
    "\n",
    "\n",
    "Standard gradient descent with $\\alpha$ learning rate. The loss L is defined with respect to the entire training set. \n",
    "\n",
    "\\begin{equation} \n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} - \\alpha \\nabla_{w} L(\\mathbf w)\n",
    "\\end{equation} \n",
    "\n",
    "Stochastic Gradient Descent (SGD), the loss $L$ is defined with respect to a minibatch of $m$ examples chosen randomly at each step. \n",
    "\n",
    "Empirical considerations: \n",
    "\n",
    "1. Small-mini batch size helps escape small local minima and computational cost of each weight update is a small constant, independent of training set size. \n",
    "2. The gradient contribution of each training example in the SGD minibatch can be computed independently, the minibatch size is often chosen so as to take maximum advantage of hardware parallelism in GPUs or TPUs. \n",
    "3. Learning rate needs to decrease over time. Choosing the right schedule is usually a matter of trial and error. \n",
    "4. Care must be taken to mitigate numerical instabilities that may arise due to overflow, underflow, and rounding error. \n",
    "\n",
    "Process of learning stops when there are diminishing returns. \n",
    "\n",
    "\n",
    "### Batch normalization \n",
    "\n",
    "### Generalization \n",
    "\n",
    "Approaches to improving generalization in deep learning include: \n",
    "\n",
    "1. Choosing the right architecture, varying number of layers, connectivity, and types of nodes \n",
    "2. Penalizing large weights \n",
    "3. Randomly perturing the values passing through the network during training \n",
    "4. Data augmentation\n",
    "\n",
    "Deeper (and narrow) networks tend to do better than shallow and wide networks for the same number of weights. \n",
    "Deep learning works well with high-dimensional data such as images, video, speech. They have to a large extent \n",
    "replaced preprocessing approaches that extracted features that prevailed prior to 2010. \n",
    "\n",
    "**Weight decay** encourages weights to become small in some ways enforcing **reguralization** i.e limiting the complexity of the model. \n",
    "\n",
    "**Dropout** \n",
    "\n",
    "At each step of training, dropout applies one step of back-propagation learning to a new version of the network that is created by deactivating a randomly chosen subset of the units. \n",
    "\n",
    "1. Introduce noise that provides robustness\n",
    "2. Approximation of large ensemble of thinned networks \n",
    "3. Paying attention to all features of the example rather than focusing on just a few \n",
    "\n",
    "Usually makes it harder to fit the training set, it is usually necessary to use a larger model and to train it for more iterations. \n",
    "\n",
    "\n",
    "**Data augmentation** \n",
    "\n",
    "Provide multiple version of the same input (for example for images: add noise, crop, rotate, scale) to increase the size of the training data and the robustness to the various transformations applied. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Graduate student descent \n",
    "\n",
    "Incremental exploratory work carried out by graduate students to figure out which architectures \n",
    "work best for which problems. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks \n",
    "\n",
    "Recurrent neural networks (RNNs) are distinct from feedforward networks in that they allow cycles in the computational graph. Each cycle has a delay i.e units may take as input a value computed from their own output at an earlier steop in the computation. This allows the RNN to have internal state or **memory**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised learning and transfer learning \n",
    "\n",
    "Unsupervised learning: \n",
    "\n",
    "1. Representation learning \n",
    "2. Generative modeling \n",
    "\n",
    "\n",
    "Joint model $P_w(\\mathbf x,\\mathbf z)$, where $\\mathbf z$ is a set of latent, unobserved variables that represent the content of the data $\\mathbf x$ in some way. \n",
    "\n",
    "A learned probability model achieves both representation learning (it has constructed meaningful $z$ vectors from the raw $\\mathbf x$ vectors) and generative modelingL if we integrate $\\mathbf z$ out of $P_{w}(\\mathbf x,\\mathbf z)$ we obtain $P_{w}(\\mathbf x)$. \n",
    "\n",
    "\n",
    "1. Probabilistic PCA \n",
    "2. Autoencoders \n",
    "3. Deep autoregressive models \n",
    "4. Generative adversarial networks \n",
    "5. Unsupervised translation \n",
    "6. Transfer learning and multitask learning\n",
    "7. ResNets\n",
    "8. U-Nets\n",
    "9. Siamese Networks\n",
    "10. Recurrent Neural Networks\n",
    "11. Resnets\n",
    "12. Transformers\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
