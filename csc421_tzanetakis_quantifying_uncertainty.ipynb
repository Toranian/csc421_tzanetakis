{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 12 - Quantifying Uncertainty \n",
    "\n",
    "### George Tzanetakis, University of Victoria \n",
    "\n",
    "\n",
    "* Laziness: too much work to completely cover every possible expection \n",
    "* Theoretical ignorance \n",
    "* Practical ignorance: missing evidence, noisy observations, unexpected changes \n",
    "\n",
    "Probability is the primary formalism for dealing with uncertainty \n",
    "\n",
    "* Degree of belief  (sentence is true or false) \n",
    "    * There is a pit at square (2,2)\n",
    "    \n",
    "\n",
    "* Ace of spades example: \n",
    "    * No knowledge (1/52) \n",
    "    * Color  (1/26) \n",
    "    * Suit (1/13 ) \n",
    "    * Show card (0 or 1) \n",
    "\n",
    "\n",
    "Note: Alternative: Fuzzy Logic – degree of truth “This towel is wet” \n",
    "            \n",
    "In this notebook we will explore discrete random variables and sampling. After defining a helper class and associated functions we will be able to create both symbolic and numeric random variables and generate samples from them. \n",
    "\n",
    "\n",
    "**IMPORTANT:** Probability is also the primary way one can connect computer and especially AI systems with other fields of science, engineering, humanities etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENTS\n",
    "\n",
    "1. [Workplan](#workplan) \n",
    "2. [Probability](#probability)\n",
    "3. [Random Variables and Sampling](#random_variables)\n",
    "4. [Joint Probability Distribution](#joint_probs)\n",
    "5. [Conditional Probabilities](#conditional_probs)\n",
    "6. [Bayes classification](#bayes_classification)\n",
    "7. [Probabilistic Wumpus World](#prob_wumpus_world)\n",
    "8. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='workplan'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKPLAN \n",
    "\n",
    "The section number is based on the 4th edition of the AIMA textbook and is the suggested\n",
    "reading for this week. Each list entry provides just the additional sections. For example the Expected reading include the sections listed under Basic as well as the sections listed under Expected. Some additional readings are suggested for Advanced. \n",
    "\n",
    "1. Basic: Sections **12.1**, **12.2**, **12.3**, **12.4**, **12.5**, **12.6**, and **Summary**\n",
    "2. Expected: Same as Basic \n",
    "3. Advanced: All the chapter including bibligraphical and historical notes \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Probablity'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probablity Introduction \n",
    "\n",
    "### Sidenote about language confusion: Monty Hall problem \n",
    "    \n",
    "    \n",
    "Three closed doors one of which hides the car of your dreams. Behind each of the other two goats. You will choose a door and win whatever is behind it. You decide on a door and announce your choice, whereas the hosts opens one of the other two doors and reveals a goat. He then asks if you would like to switch you choice or not. What should you do ? \n",
    "\n",
    "https://en.wikipedia.org/wiki/Monty_Hall_problem\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability \n",
    "\n",
    "* Primitive instictive probability \n",
    "    * Dark clouds today mean rain is likely since it has rained in the past when the clouds had that look  \n",
    "* Formal theory 17-century Correspondance between B. Pascal and P. Fermat about gambling \n",
    "\n",
    "Pervasive in all sciences. Knowledge of probability is critical for any CS practitioner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample spaces, events, worlds\n",
    "\n",
    "\n",
    "* A = {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)} \n",
    "* Rolling a seven with a pair of dice \n",
    "* Experiment gives rise to sample space \n",
    "* Discrete sample spaces (infinite but countable) \n",
    "\n",
    "* Probability distribution = associating each of the discrete outcomes with a number between 0 and 1. The sum of all the outcomes must add up to 1. \n",
    "\n",
    "\n",
    "Car-goat \n",
    "\n",
    "\n",
    "* Assume you switch S = {(1,2,3,L), (1, 3, 2, L), (2, 3, 1, W), (3, 2, 1, W)} \n",
    "* Door you choose, door hosts opens, door you switch to, win/loss \n",
    "\n",
    "Sidenote 2: Another puzzle \n",
    "\n",
    "* The king comes from a family of two children. What is the probability that the other child is his sister ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making decisions under uncertainty \n",
    "\n",
    "Suppose I believe the following: \n",
    "* P(30min  gets me there on time) = 0.04 \n",
    "* P(1 hr gets me there on time) = 0.70 \n",
    "* P( 24 hrs gets me there on time) = 0.98 \n",
    "\n",
    "* Which action to choose ? Depends on my preferencesUtility theory = represent preferences \n",
    "* Decision theory = utility theory + probability theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Theory \n",
    "\n",
    "\n",
    "* Set $\\Omega$  – the sample space \n",
    "* $\\omega \\in \\Omega$ is a sample point/atomic event, outcome, possible world \n",
    "* A probability space/model is a sample space with an assignment $P(\\Omega)$ for every point such that \n",
    "    * $0.0 \\leq P(\\omega) \\leq 1.0$\n",
    "    * $\\sum_{\\omega} P(\\omega) = 1.0$ \n",
    "    \n",
    "An event is a subset of $\\Omega$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random variables \n",
    "\n",
    "* A random variable is a function from sample points to some range e.g the reals or booleans e.g. \n",
    "* Odd(1) = true \n",
    "* P induces a probability distribution for any r.v X \n",
    "    * $P(Odd = true) = P(1) + P(3) + P(5) = \\frac{1}{2}$ \n",
    "    \n",
    "Random Variables represents a “part” of the world whose “status” is initially unknown. Each random variable has a domain that it can take on. For example, the RV Weather can have the values: sun, rain, cloud, snow. Domains can be boolen, discrete, continuous.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Probabilities are assigned over values in the domain. \n",
    "\n",
    "The notation P(Weather) denotes a vector of values for the probabilities of each individual state of the weather: \n",
    "\n",
    "* $ P(Weather = sunny) =0.65$ \n",
    "* $ P(Weather = rain) =0.25 $\n",
    "* $ P(Weather = cloudy)=0.07 $\n",
    "* $ P(Weather = snow) = 0.03 $ \n",
    "* $P(Weather) = (0.65,0.25,0.07,0.03)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposition \n",
    "\n",
    "* Think of a proposition as the event (set of sample points) where the proposition is true \n",
    "* Given boolen random variables A, BEvent a = set of sample points where A(w ) = 1 \n",
    "* Often in AI sample points are defined by the values of a set of random variables i.e., the sample space is the Cartesian product of the ranges of the variables \n",
    "* With boolean variables sample point = propositional logic model A = true, B = false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist interpretation \n",
    "\n",
    "An event’s probability is the limit of its relative frequency in a large number of trials. This connects to statistics and empirical experiments. The initial classical definition of probability was based on physical idealized symmetry (dice, coins, cards). The axiomatic formulation of probability by Kolmogorov (1903-1987) in 1933 focuses on operations on probability values rather than the initial assignment of values.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/frequentists_bayesians.png\" width=\"60%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Probability theory help us calculate unknown probabilities of events based on known probabilities of other events. How the numbers are assigned to particular events is problem and domain dependent. They can be assigned based on degrees of belief or they can be estimated by statistical frequency of occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random_varibales'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random_variables'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "\n",
    "# Random variables and sampling\n",
    "\n",
    "The probabilities associated with every possible value of a random variable consitute a probability distribution. The process of selecting a value randomly according to the probability distribution is called sampling. It can be viewed as a process of generating a sequence of random samples and it can help us better understand how a particular probabilistic model works.\n",
    "\n",
    "\n",
    "Define a helper random variable class based on the scipy discrete random variable functionality providing both numeric and symbolic RVs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Random_Variable: \n",
    "    \n",
    "    def __init__(self, name, values, probability_distribution): \n",
    "        self.name = name \n",
    "        self.values = values \n",
    "        self.probability_distribution = probability_distribution \n",
    "        if all(type(item) is np.int64 for item in values): \n",
    "            self.type = 'numeric'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (values, probability_distribution))\n",
    "        elif all(type(item) is str for item in values): \n",
    "            self.type = 'symbolic'\n",
    "            self.rv = stats.rv_discrete(name = name, values = (np.arange(len(values)), probability_distribution))\n",
    "            self.symbolic_values = values \n",
    "        else: \n",
    "            self.type = 'undefined'\n",
    "            \n",
    "    def sample(self,size): \n",
    "        if (self.type =='numeric'): \n",
    "            return self.rv.rvs(size=size)\n",
    "        elif (self.type == 'symbolic'): \n",
    "            numeric_samples = self.rv.rvs(size=size)\n",
    "            mapped_samples = [self.values[x] for x in numeric_samples]\n",
    "            return mapped_samples \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some random samples of symbolic random variables corresponding to a coin and a dice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['Alice', 'Bob']\n",
    "probabilities = [0.5, 0.5]\n",
    "coin = Random_Variable('Coin', values, probabilities)\n",
    "print(coin.name)\n",
    "samples = coin.sample(10)\n",
    "print(samples)\n",
    "\n",
    "\n",
    "def length(samples): \n",
    "    length=0\n",
    "    for x in samples: \n",
    "        length = length+1 \n",
    "    return length\n",
    "    \n",
    "print(length(samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = ['1', '2', '3', '4', '5', '6']\n",
    "probabilities = [1/6.] * 6\n",
    "probabilities = [1/6., 1/6., 1/6., 1/6., 1/6., 1/6.]\n",
    "dice = Random_Variable('dice', values, probabilities)\n",
    "samples = dice.sample(30)\n",
    "print(samples);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a numeric random variable corresponding to a dice so that we can more easily make plots and histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(1,7)\n",
    "probabilities = [1/6.] * 6\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = Random_Variable('dice', values, probabilities)\n",
    "samples = dice.sample(1000)\n",
    "print(samples[0:10])\n",
    "plt.stem(samples, markerfmt= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at a histogram of these generated samples. Notice that even with 500 samples the bars are not equal length so the calculated frequencies are only approximating the probabilities used to generate them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(samples,bins=[1,2,3,4,5,6,7],density=1, rwidth=0.5,align='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the cumulative histogram of the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples,bins=[1,2,3,4,5,6,7],density=1, rwidth=0.5,align='left', cumulative=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now estimate the frequency of the event *roll even number* in different ways. \n",
    "First let's count the number of even numbers in the generated samples. Then let's \n",
    "take the sum of the counts of the individual estimated probabilities. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also write the predicates directly using lambda notation \n",
    "samples = dice.sample(500)\n",
    "est_even = len([x for x in samples if x%2==0]) / len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_2 = len([x for x in samples if x==2]) / len(samples)\n",
    "est_4 = len([x for x in samples if x==4]) / len(samples)\n",
    "est_6 = len([x for x in samples if x==6]) / len(samples)\n",
    "print(est_even)\n",
    "# Let's print some estimates \n",
    "print('Estimates of 2,4,6 = ', (est_2, est_4, est_6))\n",
    "print('Direct estimate = ', est_even) \n",
    "print('Sum of estimates = ', est_2 + est_4 + est_6)\n",
    "print('Theoretical value = ', 0.5)\n",
    "\n",
    "\n",
    "# directly count rather than using list comprehension\n",
    "est_2 = 0 \n",
    "for x in samples: \n",
    "    if x == 2: est_2 = est_2 + 1 \n",
    "print('Estimate for 2:', est_2/len(samples))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Notice that we can always estimate the probability of an event by simply counting how many times it occurs in the samples of an experiment. However if we have multiple events we are interested in then it can be easier to calculate the probabilities of the values of invdividual random variables and then use the rules of probability to estimate the probabilities of more complex events. This is particularly useful when there are very few (or none) cases in our \"training\" data in which the event occurs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenote I: A probabilistic view of machine learning \n",
    "\n",
    "\n",
    "The basic recipe: \n",
    "\n",
    "* Describe how the data is generated and the assumptions you make using a probabilistic model \n",
    "* Estimate the parameters of the probabilistic model using available data (the learning part) \n",
    "* Use the estimated probabilistic model to perform various tasks. \n",
    "* Evaluate how well the model performs \n",
    "\n",
    "\n",
    "Some important observations: \n",
    "\n",
    "* Understanding notation in addition to the underlying concepts is important \n",
    "* Separating model from inference \n",
    "* Understanding the connection between statistics and probability \n",
    "* Thinking the generative way \n",
    "* Probabilistic modeling is all about how to calculate probabilities of events that are “hard” to estimate from probabilities of events that are “easier” to estimate \n",
    "* Focus on the basic concepts and don’t get bogged down in the implementation details and the multiple variants \n",
    "* Misleading use of language is frequently why probability problems can be difficult (for example Monty Hall). In most applications that’s not a problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidenote II \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Probabilities \n",
    "\n",
    "Let's look at some simple visualizations that can help us get some intuition about \n",
    "probabilities and statistics by looking at some specific examples. We will use a grid \n",
    "visualization to help us better undertstand the numbers. It is a simple way of visualizing a probability using a grid of colored cells. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "import numpy as np\n",
    "\n",
    "# plot a xmax by ymax grid of rectangles and use it do visualize \n",
    "# probabilities \n",
    "\n",
    "def plot_prob_grid(xmax, ymax, prob, title): \n",
    "    X = np.arange(0.5, xmax, 1)\n",
    "    Y = np.arange(0.5, ymax, 1)\n",
    "    num_grid_points = xmax * ymax \n",
    "    C = ['blue']*num_grid_points \n",
    "    grid = np.meshgrid(X, Y)\n",
    "    positions = np.vstack(list(map(np.ravel, grid)))\n",
    "    X = positions[0]\n",
    "    Y = positions[1]\n",
    "    prob_grid_points = int(prob * num_grid_points)\n",
    "    C[0:prob_grid_points] = ['orange']*prob_grid_points\n",
    "    z = figure(title=title, \n",
    "           toolbar_location='above', x_range=[0,xmax], \n",
    "           y_range=[0,ymax], toolbar_sticky=False, x_axis_location=\"below\", width=600, height=600)\n",
    "\n",
    "    z.rect(X, Y, width=1, height=1,fill_color=C, \n",
    "           line_color=\"white\")\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try visualizing a probability of 0.25 using a grid of 100x100 as well as a grid of 20x20\n",
    "output_notebook()\n",
    "z = plot_prob_grid(100,100,0.25, \"Prob = 0.25\")\n",
    "show(z)\n",
    "\n",
    "\n",
    "z = plot_prob_grid(20,20,0.25,\"Prob 0.25\")\n",
    "show(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='joint_probs'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Probability Distributions\n",
    "\n",
    "Complete set of RVs used to describe the problem can be represented as\n",
    "the joint probability distribution. For example the joint distribution \n",
    "$P(Weather,Raincoat, Season)$ can be represented as a $2 x 2 x 4$ table. \n",
    "\n",
    "### Marginal, joint and conditional \n",
    "\n",
    "|  ---   | CSC         | SENG        |\n",
    "|--------| ----------- | ----------- |\n",
    "|CSC421  |  150        | 50          |\n",
    "|SENG350 |  80         | 20          |\n",
    "\n",
    "\n",
    "\\begin{align*} \n",
    "& P(\\mbox{X is CSC}) = \\frac{150+80}{150+80+50+20} = \\frac{230}{300} \\approx 0.77  \\\\\n",
    "& P(\\mbox{X is SENG and X in CS421}) = \\\\ \n",
    "& P(SENG, CSC421) = \\frac{50}{300} \\approx 0.17 \\\\ \n",
    "& P(\\mbox{X is SENG if we know that X is in CSC421}) = \\\\ \n",
    "& P(SENG | CSC421) = \\frac{50}{200} = 0.25 \n",
    "\\end{align*} \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the same visualization to examine some probabilities \n",
    "# derived from actual data related to the Covid pandemic in BC \n",
    "\n",
    "covid_deaths_bc = 2000 \n",
    "bc_population = 5000000\n",
    "covid_recovered_bc = 164000\n",
    "covid_death_perc_bc = covid_deaths_bc / bc_population\n",
    "print(covid_death_perc_bc)\n",
    "covid_recovered_perc_bc = covid_recovered_bc / bc_population\n",
    "\n",
    "z = plot_prob_grid(100,100,covid_death_perc_bc, 'Covid Deaths BC')\n",
    "show(z)\n",
    "\n",
    "z = plot_prob_grid(100,100,covid_recovered_perc_bc, 'Covid Recovered BC')\n",
    "show(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "When we have a probabilistic model we can make predictions, learn about the values of some random variables given the values of others, and in general, answer any possibly questions that can be stated about the random variables. The probablistic model expresses the set of assumptions we are making about the problem we are tryng to solve and our uncertainty about them is expressed through probabilities. Typically we will know the values of some random variables in our model (evidene) and based on this knowledge we will want to infer something about the probability distribution of some other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Rule \n",
    "\n",
    "We have seen that: \n",
    "* $P(hasLyrics,genre) = P(genre)P(hasLyrics/genre)$ \n",
    "\n",
    "This is an example of the product rule: P(A,B) = P(A)P(B|A)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum Rule \n",
    "\n",
    "We can sum the joint probabilities for all possible values of genre to “eliminate” that variable. \n",
    "\n",
    "* $\\sum_{hasLyrics} P(hasLyrics,genre = country) = P(genre = country)$. \n",
    "\n",
    "More generally using short-hand notation we can express that this holds for all values of genre : \n",
    "\n",
    "* $\\sum_P{hasLyrics} P(hasLyrics,genre) = P(genre)$.\n",
    "\n",
    "\n",
    "More generally the sum rule of probability states: $\\sum_{B} P(A,B) = P(A)$ \n",
    "\n",
    "In this context, the distribution $P(A)$ is known as the marginal distribution for $A$ and the act of summing out $B$ is called marginalisation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum and product rules \n",
    "\n",
    "The sum and product rules are very general. They apply not just when $A$ and $B$ are binary random variables, but also when they are multi-state random variables, and even when they are continuous (in which case the sums are replaced by integrations). Furthermore, A and B could each represent sets of several random variables. For example if $B = C,D$: \n",
    "\n",
    "* $P(A,C,D) = P(A)P(C,D|A)$ \n",
    "* $P(A) = \\sum_{C} \\sum_{D} P(A,C,D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using full joint distribution\n",
    "\n",
    "\n",
    "Let's consider another example where the full joint distribution $2 x 2 x 2$ is given. \n",
    "\n",
    "\n",
    "\n",
    "|---| toothache and catch    | toothache and not catch | not toothache and catch | not toothache and not catch | \n",
    "|---|-------   | ----------| ------| ----------|\n",
    "|cavity | 0.108 | 0.012 | 0.072 | 0.008 | \n",
    "| not cavity | 0.016 | 0.064 | 0.144 | 0.576 | \n",
    "\n",
    "\n",
    "Direct way to evalute the probability of any proposition: \n",
    "* Identify the possible worlds in which a proposition is true and add up their probabilities \n",
    "* $P(cavity \\lor toothache) = 0.108 + 0.012 + 0.072 + 0.008 + 0.016+ 0.064 = 0.28$ \n",
    "* **Marginal probability** of cavity: \n",
    "* $P(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conditional_probs'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional probabilities \n",
    "\n",
    "We explore conditional probabilities using an example from music. Imagine that you have a collection of tracks consisting of two genres: country and jazz. Some tracks have lyrics and some have not i.e they are instrumental. It makes sense that the probability of a song being instrumental depends on whether the song is jazz or country. This can be modeled through conditional probabilities.\n",
    "\n",
    "We can simulate the generation process of conditional probabilities by appropriately sampling from three random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we introduce another random variable hasLyrics with two values: no and yes. We expect that more country songs will have lyrics than jazz songs. That means that the probability distribution of hasLyrics depends on whether the genre is country or jazz. This is known as a conditional probability distribution and is notated as follows: $P(hasLyrics = no/genre = jazz) = 0.9$ This implies that $P(hasLyrics = yes|genre = jazz) = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If genre = country then we have: $P(hasLyrics = no|genre = country) = 0.2$ We can use the short-hand notation $P(hasLyrics|genre)$ to denote the conditional probability distribution that in this case can be specified by providing four probabilities (or two using normalization). We will call these numbers and in general any numbers used to “specify” a particular probabilistic model parameters and use $\\theta$ to denote a vector containing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display all the relevant probabilities using a conditional probability table. Notice that the sum of row entries must be equal to 1 but NOT the sum of column entries. \n",
    "\n",
    "| Lyrics/Genre | no  | yes |\n",
    "|--------------|-----|-----|\n",
    "| country      | 0.2 | 0.8 |\n",
    "| jazz         | 0.9 | 0.1 |\n",
    "\n",
    "|              | jazz | country | \n",
    "|--------------|---------|------|\n",
    "| Genre        | 0.3   | 0.7 |\n",
    "----------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence and conditional independence \n",
    "\n",
    "Let's look at another example: \n",
    "<br> \n",
    "$A$ and $B$ are independent iff $P(A | B) = P(A)$ or $P(B|A) = P(B)$ or $P(A,B) = P(A) P(B)$\n",
    "\n",
    "Absolute independence is powerful but rare. \n",
    "\n",
    "\n",
    "* $P(catch | toothache, cavity) = P(catch| cavity)$\n",
    "* $P(catch | toothache, not cavity) = P(catch| not cavity)$ \n",
    "\n",
    "Catch is conditionally independent of Tootchache given Cavity (note the use of capital letters to indicate random variables: \n",
    "\n",
    "* $P(Catch | Toothache, Cavity) = P(Catch|Cavity)$\n",
    "\n",
    "\n",
    "**MOST BASIC AND ROBUST FORM OF KNOWLEDGE ABOUT UNCERTAINTY**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about notation** \n",
    "\n",
    "Frequently when notating a conditional probablity distribution the short hand $P(hasLyrics|genre)$ is used. Conceptually this expands to all possible combinations of values of the two random variables involved. Also some times when the values of random variables in a problem are unique the name of the random variable is omitted i.e P(country) instead of P(genre = country). It is important to keep in mind these conventions as our examples get more complicated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples to generate \n",
    "num_samples = 1000\n",
    "\n",
    "## Prior probabilities of a song being jazz or country \n",
    "values = ['country', 'jazz']\n",
    "probs = [0.7, 0.3]\n",
    "genre = Random_Variable('genre',values, probs)\n",
    "\n",
    "# conditional probabilities of a song having lyrics or not given the genre \n",
    "values = ['no', 'yes']\n",
    "probs = [0.9, 0.1] \n",
    "lyrics_if_jazz = Random_Variable('lyrics_if_jazz', values, probs)\n",
    "\n",
    "values = ['no', 'yes']\n",
    "probs = [0.2, 0.8]\n",
    "lyrics_if_country = Random_Variable('lyrics_if_country', values, probs)\n",
    "\n",
    "# conditional generating proces first sample prior and then based on outcome \n",
    "# choose which conditional probability distribution to use \n",
    "\n",
    "random_lyrics_samples = [] \n",
    "for n in range(num_samples): \n",
    "    # the 1 below is to get one sample and the 0 to get the first item of the list of samples \n",
    "    random_genre_sample = genre.sample(1)[0]\n",
    "    # depending on the outcome of the genre sampling sample the appropriate \n",
    "    # conditional probability \n",
    "    if (random_genre_sample == 'jazz'): \n",
    "        random_lyrics_sample = (lyrics_if_jazz.sample(1)[0], 'jazz')\n",
    "        # random_lyrics_sample = (lyrics_if_jazz.sample(1)[0])\n",
    "\n",
    "    else: \n",
    "        random_lyrics_sample = (lyrics_if_country.sample(1)[0], 'country')\n",
    "        # random_lyrics_sample = (lyrics_if_country.sample(1)[0])\n",
    "\n",
    "    random_lyrics_samples.append(random_lyrics_sample)\n",
    "\n",
    "# output 1 item per line and output the first 20 samples \n",
    "for s in random_lyrics_samples[0:100]: \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have generated samples of whether the song has lyrics or not. Above I have also printed the associated genre label. In many probabilistic modeling problems some information is not available to the observer. For example we could be provided only the yes/no outcomes and the genres could be \"hidden\".\n",
    "\n",
    "Now let's use these generated samples to estimate probabilities of the model. Basically we pretend that we don't know the parameters and estimate them directly by frequency counting through the samples we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First only consider jazz samples \n",
    "jazz_samples = [x for x in random_lyrics_samples if x[1] == 'jazz']\n",
    "for s in jazz_samples[0:20]: \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the samples that are jazz we can simply count the lyrics yes and lyrics no entries and divide them by the total number of jazz samples to get estimates of the conditional probabilities. Think about the relationships: we can use the data to estimate the parameters of a model (learning), we can use the model to generate samples (generation), and we can use the model to calculate probabilities for various events (inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the probability of a song being jazz if we know that it is instrumental is 0.66.$$\n",
    "P(genre = jazz | hasLyrics = no) = \\frac{0.3 * 0.9}{0.3 * 0.9 + 0.7 * 0.2} = 0.66\n",
    "$$\n",
    "\n",
    "This is based on our knowledge of probabilities. If we have some data we can also estimate this probability directly. This is called approximate inference in contrast to the exact inference of $0.66$. When problems become complicated exact inference can become too costly to compute while approximate inference can provide reasonable answers much faster. We will see that later when examining probabilistic graphical models. As you can see in this case both the exact and approximate inference probability estimates are relatively close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_no_if_jazz = len([x for x in jazz_samples if x[0] == 'no']) / len(jazz_samples)\n",
    "est_yes_if_jazz = len([x for x in jazz_samples if x[0] == 'yes']) / len(jazz_samples)\n",
    "print(est_no_if_jazz, est_yes_if_jazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = [x for x in random_lyrics_samples if x[0] == 'no']\n",
    "est_jazz_if_no_lyrics = len([x for x in no_samples if x[1] == 'jazz']) / len(no_samples)\n",
    "print(est_jazz_if_no_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that The process of obtaining revised probability distributions after the values of some random variables have been observed, is called inference. Let’s look at an example. We know that the probability of a song is jazz is $30\\%$. Suppose that we observe that the song does not have lyrics. How does this evidence affect the probability that the song is jazz ? We have:\n",
    "\n",
    "\n",
    "* $P(genre = jazz|hasLyrics = no) = \\frac{0.3 ∗ 0.9}{0.3∗0.9+0.7∗0.2} \\approx 0.66 $ \n",
    "\n",
    "\n",
    "Notice that this posterior probability after incorporating evidence is more than twice the original prior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule \n",
    "\n",
    "* $P(a|b) = P(b|a) P(a) / P(b) $ \n",
    "\n",
    "How can you prove it ? \n",
    "\n",
    "In distribution form (random variables - basically a template generating specific equations):  \n",
    "* $P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$\n",
    "* $P(A=1, B=H) = \\frac{P(B=H|A=1)P(A=1)}{P(B=H)}$\n",
    "\n",
    "Diagnostic probability from causal probability\n",
    "\n",
    "* $P(Cause |Effect) = P(Effect|Cause) P(Cause) / P(Effect)$\n",
    "* M menigitis, S stiff neck with: \n",
    "\n",
    "* $P(M) = 0.0001, P(S|M) = 0.8 P(S) = 0.1 P(M|S) = ?$\n",
    "\n",
    "* Why is this useful ? Are diagnostic rules or causal rules harder to estimate ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability Notation \n",
    "\n",
    "\\begin{eqnarray}\n",
    "P(X = x_1, Y = y_1) = P(X = x_1 / Y = y_1) P(Y=y1) \\\\ \n",
    "P(X = x_1, Y = y_2) = P(X = x_1 / Y = y_2) P(Y=y2) \\\\ \n",
    "\\dots \n",
    "\\end{eqnarray}\n",
    "can be combined with the notation denoting a set of equations: \n",
    "\\begin{equation} \n",
    "{\\bf P}(X,Y) = {\\bf P}(X/Y) {\\bf P}(Y)\n",
    "\\end{equation} \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes example \n",
    "\n",
    "Suppose $C$ is a rv corresponding to people with covid in a population \n",
    "and $O$ is a rv corresponding to a particular age group (let's say above 70). \n",
    "We have the following data: \n",
    "$P(C) = 0.001$, $P(O|C) = 0.9$, $P(O|\\hat C)=0.21$. P(C|O) corresponds to the probability of getting covid if you are old and can be calculated using the Bayes theorem: \n",
    "\n",
    "\\begin{equation} \n",
    "P(C/O) = \\frac{P(O/C)P(C)}{P(O)} = \\frac{0.0009}{0.9 * 0.001 + 0.21 * 0.999} = 0.0043 \n",
    "\\end{equation} \n",
    "\n",
    "**NOTE: these numbers are made up and are not connected to actual Covid data**\n",
    "\n",
    "\n",
    "Bayes theorem allows us to ``choose'' in a particular problem the\n",
    "conditional probabilities that are easier to calculate from data. For example it\n",
    "is easier to obtain the probability that someone who has covid \n",
    "is old than the probability that someone who is old has lung covid. \n",
    "It also allows us to quickly update probabilities when knowledge about the problem \n",
    "changes. For example suppose that the number of cases increase significantly after \n",
    "relaxing some public health measures. This for example could affect $P(C)$ without affecting the other numbers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes Model \n",
    "\n",
    "Commonly occuring pattern in which a single cause directly influences a number of effects all of which are conditionally independent. \n",
    "\n",
    "\n",
    "<img src=\"images/naive_bayes_net.png\" width=\"60%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bayes_classification'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classification \n",
    "\n",
    "\n",
    "* $P(Y/X) = \\frac{P(X/Y)P(Y)}{P(X)}$ \n",
    "\n",
    "where Y is the class label and X is the feature vector. Notice that this is a set of equations, one for each class label in Y. Therefore there will be L posterior probabilities one for each class. To classify a test instance a Bayesian classifier computes these posterior probabilities and selects the class label corresponding to the maximum posterior. Main challenge becomes how to estimate P(X/Y) from the labeled training samples. For each class the corresponding training samples are used to estimate the parameters of the corresponding pdfs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classification \n",
    "\n",
    "We look at the problem of classifying songs to three genres (rap, rock and country) based on a simple binary bag of words representation. First we load the data and then we take a look at it. Using our implementation of discrete random variables we generate new random songs. Finally we show how classification can be performed using Bayes Rule. The data comes for the lyrics of songs from the Million Song DataSet and was created for an assignment in my course on MIR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data layout and visualization\n",
    "\n",
    "The data layout and the way the classifier is implemented is not general and not optimal but done for pedagogical purposes. Each genre consists of 1000 tracks and the matrix containing the data is ordered by genre. That way the instances corresponding to a genre can easily be found by the index without having to check the class label as would be the case with a general classifier.\n",
    "\n",
    "We have created a dictionary of 30 words by taking the 10 \"best\" words based on tf-idf score for each genre. Each track is represented by a binary vector (a row in the matrix) with ones for dictionary words that are in the track and 0 for words that are not. So the matrix is 3000 instances (3 * 1000 per genre) by 30 for each word in the dictionary. When visualized one can observe the block structure that shows that the the rap tracks have a lot of words from the first 10 words in the dictionary that are characteristic of rap.\n",
    "\n",
    "**NOTE** the data is conveniently arranged for this visualization. In an actual problem the rows of this matrix would be shuffled and the block structure would not be visible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# load some lyrics bag of words data, binarize, separate matrix rows by genre \n",
    "data = np.load('data/data.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data['arr_0']\n",
    "a[a > 0] = 1\n",
    "labels = np.load('data/labels.npz', allow_pickle=True)\n",
    "labels = labels['arr_0']\n",
    "dictionary = pickle.load(open('data/dictionary.pck','rb'), encoding='latin1')\n",
    "word_indices = [  41, 1465,  169,  217, 1036,  188,  260,  454,  173,  728,  163,\n",
    "        151,  107,  142,   90,  141,  161,  131,   86,   73,  165,  133,\n",
    "         84,  244,  153,  126,  137,  119,   80,  224]\n",
    "words = [dictionary[r] for r in word_indices]\n",
    "\n",
    "# binary row vectors separate by genre (rap, rock, country)\n",
    "ra_rows = a[0:1000,:]\n",
    "ro_rows = a[1000:2000,:]\n",
    "co_rows = a[2000:3000,:] \n",
    "print(ra_rows.shape, ro_rows.shape, co_rows.shape)\n",
    "plt.imshow(a, aspect='auto', cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the 30-dimensional word probability vector for each genre\n",
    "\n",
    "\n",
    "Let's calculate the word probability vector for each genre and then look at the most probable words for each genre in our data as well as how particular songs are represented as bag of words. We can calculate the probabilities of each word in the dictionary for the songs in each genre by summing the columns of the part of the matrix that corrsponds to each genre. As some words might not appear at all I have added 1.0 to both the numerator and denominator. This is a simple form of what's called additive smoothing which is a common technique to avoid zeros for any class conditional probabilities that would lead to the whole likelihood being zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate word counts for each genre \n",
    "word_probs_ra = (ra_rows.sum(axis=0).astype(float) + 1.0) / (len(ra_rows)+1.0)\n",
    "word_probs_ro = (ro_rows.sum(axis=0).astype(float) + 1.0) / (len(ro_rows)+1.0)\n",
    "word_probs_co = (co_rows.sum(axis=0).astype(float) + 1.0) / (len(co_rows)+1.0)\n",
    "\n",
    "# Let's llok at the word probabitites for rap music \n",
    "for w in zip(word_probs_ra, words): \n",
    "    print(w)\n",
    "print('------')\n",
    "for w in zip(word_probs_ro, words): \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking out the words in some songs using the binary representation\n",
    "\n",
    "Each row of the feature matrix contains ones for each word that is present in the song. We can view the words of \n",
    "any particular song by mapping these ones using the dictionary of words. Let's view the words in the 20th track (row of the matrix) of each genre and then look at track 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at the bag of words for three particular songs \n",
    "track_id = 20\n",
    "print(track_id)\n",
    "print(\"RAP for trackid:\",[words[i] for i,r in enumerate(ra_rows[track_id]) if r==1])\n",
    "print(\"ROCK for trackid:\",[words[i] for i,r in enumerate(ro_rows[track_id]) if r==1])\n",
    "print(\"COUNTRY for trackid:\",[words[i] for i,r in enumerate(co_rows[track_id]) if r==1])\n",
    "\n",
    "track_id = 250 \n",
    "print(track_id)\n",
    "print(\"RAP for trackid:\",[words[i] for i,r in enumerate(ra_rows[track_id]) if r==1])\n",
    "print(\"ROCK for trackid:\",[words[i] for i,r in enumerate(ro_rows[track_id]) if r==1])\n",
    "print(\"COUNTRY for trackid:\",[words[i] for i,r in enumerate(co_rows[track_id]) if r==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the k most probable words for each genre based on the data we have \n",
    "k = 5\n",
    "[[words[x] for x in np.argpartition(word_probs_ra, -k)[-k:]],\n",
    " [words[x] for x in np.argpartition(word_probs_ro, -k)[-k:]],\n",
    " [words[x] for x in np.argpartition(word_probs_co, -k)[-k:]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random songs based on our simplified representation\n",
    "\n",
    "Now let's generate some random songs represented as bag of words using the calculated word probabilities for each genre. This way we can understand better the assumptions and simplifications of this model. I simply generate 30 random number and then depending on the class-conditional probabilities for a particular genre if the number is great than the random number the corresponding word is selected for generation. This gives us a clear idea of what assumptions this Binomial Naive Bayes classifier makes. Running the cell multiple times show the variation we get from this very simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Random rap', [w for (i,w) in enumerate(words) if np.greater(word_probs_ra, np.random.rand(30))[i]])\n",
    "print('Random rock', [w for (i,w) in enumerate(words) if np.greater(word_probs_ro, np.random.rand(30))[i]])\n",
    "print('Random country', [w for (i,w) in enumerate(words) if np.greater(word_probs_co, np.random.rand(30))[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the calculated word probabilities to make a classifier\n",
    "\n",
    "Now let's look at classifying songs using a naive Bayes Bernoulli classifier. When the representation is binary vectors indicating absense or presence of words it is called a Bernoulli Naive Bayes. If the times a word appears in a document affect the classification it is called a Multinomial text classifier.\n",
    "\n",
    "To make a classification decision we simply calculate the likelihood for each genre independently by taking the products of the genre dependent word probabilities. The genere with the highest likelihood is selected as the predicted class. In a more realistic implementation log-likelihoods would be used to avoid problems with small numbers. Notice that when a word is absent the probability it is absent (1 - the probability it is present) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcuate likelihood separately for each word \n",
    "# using naive bayes assumption and multiply \n",
    "# typically a sum of log-likelihoods is used \n",
    "# rather than a multiplication. \n",
    "def likelihood(test_song, word_probs_for_genre): \n",
    "    probability_product = 1.0 \n",
    "    for (i,w) in enumerate(test_song): \n",
    "        if (w==1): \n",
    "            probability = word_probs_for_genre[i]\n",
    "        else: \n",
    "            probability = 1.0 - word_probs_for_genre[i]\n",
    "        probability_product *= probability \n",
    "    return probability_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the trained classifier to predict\n",
    "\n",
    "Now that we have a function to compute the likelihood given the parameters of a particular model in this case the model parameters are the probabilities for each word. We have three models to compare one for each genre. Given a test song we compute the three likelihoods and select the largest. We can randomly select a track from the country rows and then apply our predict function to see what it does. If you run the cell multiple times you will see that for most country tracks the prediction is correct but mistakes are made occassionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test_song): \n",
    "    scores = [likelihood(test_song, word_probs_ra), \n",
    "             likelihood(test_song, word_probs_ro),\n",
    "             likelihood(test_song, word_probs_co)]\n",
    "    labels = ['rap', 'rock', 'country']\n",
    "    return labels[np.argmax(scores)]\n",
    "\n",
    "\n",
    "# predict a random country track \n",
    "track_id = np.random.randint(1000)\n",
    "print(\"Random track id\", track_id)\n",
    "test_song = co_rows[track_id]\n",
    "print(predict(test_song))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a simple evaluation of our classifier\n",
    "\n",
    "\n",
    "We can now write a function that given a test set and associated ground truth lables runs our Bernoulli classifier and calculates the associated classification accuracy. We can now check how well the classifier does for each subset of the data corresponding to the three genres. Using the data used to trained the classifier for testing as we do here is a methodological mistake and in a more realistic scenario or application a separate dataset would be used for testing and the processing could be repeated multiple times using a scheme like k-fold cross-validation. As the purpose of this notebook is to illustrate how probabilities are used to create a Naive Bayes classifier I don't bother with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_set(test_set, ground_truth_label): \n",
    "    score = 0 \n",
    "    for r in test_set: \n",
    "        if predict(r) == ground_truth_label: \n",
    "            score += 1\n",
    "    # convert to percentage \n",
    "    return score / 10.0 \n",
    "\n",
    "# Let's evaluate how well our classifier does on the training set \n",
    "# A more proper evaluation would utilize cross-validation \n",
    "\n",
    "print(\"Rap accuracy% = \", predict_set(ra_rows, 'rap'))\n",
    "print(\"Rock accuracy% = \", predict_set(ro_rows, 'rock'))\n",
    "print(\"Country accuracy% = \", predict_set(co_rows, 'country'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes in general\n",
    "\n",
    "This notebooks explores how a simple probabilistic model based on a binary representation for a bag of words can be used for classification. This is a toy example with a lot of assumptions and conventions that make things easier in terms of implementation. In an actual implementation the number of words in the dictionary would not be given but calculated from the data, the instances would be shuffled so to calculate the probabilities the class output field of every instance would have to be examined. The number of classes would not be fixed and loops for iterating over classes and over attributes would be written. In the computation of the likelihood a log-likelihood would be used instead and the list goes on. You can find more information about this in most textbook that describe Naive Bayes classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prob_wumpus_world'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Wumpus World \n",
    "\n",
    "\n",
    "<img src=\"images/prob_wumpus.png\" width=\"75%\"/>\n",
    "\n",
    "\n",
    "* Pits cause breezes in neighboring squares\n",
    "* Each square (except [1,1] ) contains a pit with probability 0.2\n",
    "* Random variables P[i,j] for each square B[i,j] for each square that is breezy \n",
    "* Please work through examples from textbook\n",
    "\n",
    "Suppose that there is a breeze in [1,2] and [2,1]. The possible next moves \n",
    "are [1,3],[2,2], and [3,1]. Each of those can potentially have a pit. \n",
    "Is there one that is \"safer\" in a probabilistic sense to move to? \n",
    "\n",
    "\n",
    "The full joint distribtion is ${\\bf P}(P_{1,1}, \\dots, P_{4,4}, B_{1,1}, B_{1,2},B_{2,1})$. Note: we only consider the B variables that are observed. Applying the product rule we get: \n",
    "$$ \n",
    "{\\bf P}(P_{1,1}, \\dots, P_{4,4}, B_{1,1}, B_{1,2},B_{2,1}) = {\\bf P}((B_{1,1}, B_{1,2},B_{2,1})|{\\bf P}(P_{1,1}, \\dots, P_{4,4})){\\bf P}(P_{1,1}, \\dots, P_{4,4})\n",
    "$$\n",
    "\n",
    "The first term is the conditional probability distribution of a breeze configuration, given the pit configuration; its values are 1 if the breezes are adjacent to the pits and 0 otherwise. The second term \n",
    "is the prior probability of a pit configuration. Each square contains a pit with a probability of $0.2$, independetly of the other squares: \n",
    "\n",
    "$$\n",
    "{\\bf P}(P_{1,1}, \\dots, P_{4,4})=\\prod_{i,j=1,1}^{4,4}{\\bf P}(P_{i,j})\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of a particular configuration with exactly $n$ pits can be \n",
    "expressed as follows: \n",
    "$$ \n",
    "{\\bf P}(P_{1,1}, \\dots, P_{4,4}) = 0.2^{n} \\times 0.8^{16-n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the situation in the Figure above, the evidence consists of the observed breeze (or its absense) in each square that is visited, combined with the fact that each such square contains no pit. We abbreviate these facts as $b=\\neg b_{1,1} \\land b_{1,2} \\land b_{2,1}$ and \n",
    "$known = \\neg p_{1,1} \\land \\neg p_{1,2} \\land \\neg p_{2,1}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query\n",
    "\n",
    "How likely is that [1,3] contains a pit given the observations so far? \n",
    "$${\\bf P}(P_{1,3}|known,b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can answer this query by summing over the entries from the full joint distribution.Let $unknown$ be the set of $P_{i,j}$ variables for squares other than the $known$ squares and the query square $[1,3]$. Then we have:\n",
    "\n",
    "$$ \n",
    "{\\bf P}(P_{1,3}|known,b) = \\alpha\\sum_{unknown}{\\bf P}(P_{1,3}, unknown, known, b)\n",
    "$$ \n",
    "\n",
    "There are 12 unknown squares therefore this summation contains $2^{12}=4096$ terms. In general, summations like this grow exponentially with the number of squares (or variables more generally). \n",
    "\n",
    "From the rules of the Wumpus world it is clear that many squares are irrelevant to the problem. \n",
    "We can formalize this intuition by taking advantage of conditional independence and make solving the problem easier i.e have less terms in the sum. The notation gets a bit complicated but if you take your time you will see it basically consists of carefully formulating subsets of the variables and using the sum and product rules with conditional independence. \n",
    "\n",
    "Let $frontier$ be the pit variables (other than the query variable) that are adjacent to the visited squares. In our case, these will $[2,2]$ and $[3,1]$. Let $other$ be the pit variables for the $10$ other unknown squares. \n",
    "\n",
    "**KEY INSIGHT:** the observed breezes are *conditionally independent* of the other variables, given the known, frontier, and query variables.\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "{\\bf P}(P_{1,3} | known, b) &= \\alpha \\sum_{unknown} {\\bf P}(P_{1,3}, known, b, unknown) \\\\ \n",
    "&= \\alpha \\sum_{unknown} P(b | P_{1,3}, known, unknown){\\bf P}(P_{1,3}, known, unknown) \\\\\n",
    "&= \\alpha \\sum_{frontier}\\sum_{other}{\\bf P}(b|P_{1,3}, known, frontier, other){\\bf P}(P_{1,3}, known, frontier, other) \\\\ \n",
    "&= \\alpha \\sum_{frontier}\\sum_{other}{\\bf P}(b|P_{1,3}, known, frontier){\\bf P}(P_{1,3}, known, frontier, other)\n",
    "\\end{aligned} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step uses **conditional independence**: $b$ is conditionally independent of $other$ given $known$, $P_{1,3}$, and $frontier$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\begin{aligned}\n",
    "{\\bf P}(P_{1,3} | known, b) \\\\\n",
    " &= \\alpha \\sum_{frontier} {\\bf P}(b | P_{1,3}, known, frontier) \\sum_{other} {\\bf P}(P_{1,3}, known, frontier, other) \\\\ \n",
    "&= \\alpha P(known) {\\bf P}(P_{1,3}) \\sum_{frontier}{\\bf P}(b|P_{1,3},known,frontier)P(frontier)\\sum_{other}P(other) \\\\\n",
    "&= \\alpha'{\\bf P}(P_{1,3})\\sum_{frontier} {\\bf P}(b|P_{1,3}, known,frontier)P(frontier)\n",
    "\\end{aligned} \n",
    "$$\n",
    "\n",
    "The last step folds P(known) into the normalizing constant and uses the fact that $\\sum_{other}P(other) = 1$.  \n",
    "\n",
    "There are just 4 terms in the summation over the frontier variables $P_{2,2}$ and $P_{3,1}$. The use of independence and conditional independence has completely eliminated the other squares for consideration. Notice that the expression ${\\bf P}(b|P_{1,3}, known, frontier)$ is $1$ when the frontier is consistent with the breeze observations and $0$ otherwise. Thus, for each value of $P_{1,3}$ we sum over the *logical models* for the frontier variables that are consistent with the known facts. \n",
    "\n",
    "The models and their associated prior probabilities are shown below. \n",
    "\n",
    "<img src=\"images/consistent_models_prob_wumpus.png\" width=\"75%\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore have: \n",
    "$$\n",
    "{\\bf P}(P_{1,3}|known,b) = \\alpha'<0.2 (0.04 + 0.16 + 0.16), 0.8(0.04 + 0.16))> \n",
    "\\approx <0.31, 0.69> \n",
    "$$\n",
    "\n",
    "Because of symmetry we can also infer that $P_{1,3}$ is also $0.31$. A similar \n",
    "calculation shows that $[2,2]$ contains a pit with roughly $0.86$ probability.\n",
    "\n",
    "Therefore the wumpus agent should definitely avoid $[2,2]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p13 = (0.2 * (0.04 + 0.16 + 0.16))\n",
    "not_p13 = (0.8 * (0.04 + 0.16))\n",
    "\n",
    "print(p13,not_p13)\n",
    "print(p13/(p13 + not_p13), not_p13 / (p13 + not_p13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "* Uncertainty arises because of finite resources and sensor noise. It is inescapable in complex, non-deterministic, or partially observable environments. \n",
    "* Probabilities express the agent's inability to reach a definite decision regarding the truth of a sentence. Probabilities summarize the agent's beliefs relative to the current evidence.\n",
    "* Decision theory combines the agent's beliefs and goals, definiting the best action as the one that maximizes expected utility\n",
    "* Basic probability statements include **prior probabilities** and **conditional probabilities** over simple and complex propositions.\n",
    "* The axioms of probability constrain the possible assignments of probabilities to propositions. An agent that violates the axioms must behave irrationally in some cases.\n",
    "* The **full joint probability distribution** specifies the probability of each coimplete assignment of values to random values. It is usually too large to create or use in its explicit form, but when it is available it can be used to answer queries simply by adding up entries for possible worlds corresponding to the query propositions.\n",
    "* **Absolute independence** between subsets of random variables allows the full joint distribution to be factored into smaller joint distribution, greatly reducing its complexity. Absolute independence seldom occurs in practice.\n",
    "* **Bayes' rule** allows unknown probabilities to be computed from known conditional probabilities usually in the causal direction. Applying Bayess rule with many pieces of evidence runs into the same scaling problems as does the full joint distribution.\n",
    "* **Conditional independence** brought about by direct causal relationships in the domain might allow the full joint distribution to be factored into smaller, conditional distributions. The **naive Bayes** model assumes the conditional independence fo all effect variables, given a single cause variable, and grows linearly with the number of effects.\n",
    "* A wumpus-world agent can calculate probabilities for unobserved aspects of the world, thereby improving on the decisions of a purely logical agent. Conditional independence makes these calculations tractable.\n",
    "*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
